{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_normalized_data():\n",
    "    print(\"Reading  data...\")\n",
    "    df = pd.read_csv('train.csv')\n",
    "    data = df.values.astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0] #label\n",
    "\n",
    "    Xtrain = X[:-1000]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest  = X[-1000:]\n",
    "    Ytest  = Y[-1000:]\n",
    "\n",
    "    # let's normalize the data\n",
    "    mu = Xtrain.mean(axis=0)\n",
    "    std = Xtrain.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    Xtrain = (Xtrain - mu) / std\n",
    "    Xtest = (Xtest - mu) / std\n",
    "    return Xtrain, Xtest, Ytrain, Ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_forward(X, W, b):\n",
    "    # softmax\n",
    "    a = X.dot(W) + b\n",
    "    expa = np.exp(a)\n",
    "    y = expa / expa.sum(axis=1, keepdims=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(p_y):\n",
    "    return np.argmax(p_y, axis=1)\n",
    "\n",
    "\n",
    "def cal_cal_error_rate(p_y, t):\n",
    "    prediction = predict(p_y)\n",
    "    return np.mean(prediction != t)\n",
    "\n",
    "\n",
    "def calculate_cost(p_y, t):\n",
    "    tot = t * np.log(p_y)\n",
    "    return -tot.sum()\n",
    "\n",
    "\n",
    "def gradW(t, y, X):\n",
    "    #default GD equation for weights \n",
    "    return X.T.dot(t - y)\n",
    "def gradb(t, y):\n",
    "    #default GD equation for bias \n",
    "    return (t - y).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2indicator(y):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    ind = np.zeros((N, 10))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fun_GradientDecent():\n",
    "    Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "    print(\"Starting logistic regression...\")\n",
    "    # lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "\n",
    "    # convert Ytrain and Ytest to (N x K) matrices of indicator variables\n",
    "    N, D = Xtrain.shape\n",
    "    Ytrain_ind = y2indicator(Ytrain)\n",
    "    Ytest_ind = y2indicator(Ytest)\n",
    "\n",
    "    W = np.random.randn(D, 10) / np.sqrt(D)\n",
    "    b = np.zeros(10)\n",
    "    LL = []\n",
    "    LLtest = []\n",
    "    CRtest = []\n",
    "\n",
    "    lr = 0.00001 #Learning rate\n",
    "    reg = 1 #regularisation parameter\n",
    "    \"\"\"\n",
    "    let's fix the learning rate a 0.00004 - not too high not too low\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(500):\n",
    "        p_y = softmax_forward(Xtrain, W, b)\n",
    "        # print \"p_y:\", p_y\n",
    "        ll = calculate_cost(p_y, Ytrain_ind)\n",
    "        LL.append(ll)\n",
    "\n",
    "        p_y_test = softmax_forward(Xtest, W, b)\n",
    "        lltest = calculate_cost(p_y_test, Ytest_ind)\n",
    "        LLtest.append(lltest)\n",
    "        \n",
    "        err = cal_cal_error_rate(p_y_test, Ytest)\n",
    "        CRtest.append(err)\n",
    "        \n",
    "        \"\"\"\n",
    "        updating the parameter sets \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        W += lr*(gradW(Ytrain_ind, p_y, Xtrain) - reg*W)\n",
    "        b += lr*(gradb(Ytrain_ind, p_y) - reg*b)\n",
    "        if i % 10 == 0:\n",
    "            print(\"calculate_cost at iteration %d: %.6f\" % (i, ll))\n",
    "            print(\"Error rate:\", err)\n",
    "\n",
    "    p_y = softmax_forward(Xtest, W, b)\n",
    "    print(\"Final error rate is :\", cal_cal_error_rate(p_y, Ytest))\n",
    "    iters = range(len(LL))\n",
    "    plt.plot(iters, LL, iters, LLtest)\n",
    "    plt.show()\n",
    "    plt.plot(CRtest)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading  data...\n",
      "Performing logistic regression...\n",
      "calculate_cost at iteration 0: 102084.654335\n",
      "Error rate: 0.842\n",
      "calculate_cost at iteration 10: 18301.010436\n",
      "Error rate: 0.124\n",
      "calculate_cost at iteration 20: 11799.351558\n",
      "Error rate: 0.063\n",
      "calculate_cost at iteration 30: 11076.081544\n",
      "Error rate: 0.062\n",
      "calculate_cost at iteration 40: 10713.694766\n",
      "Error rate: 0.061\n",
      "calculate_cost at iteration 50: 10473.607942\n",
      "Error rate: 0.062\n",
      "calculate_cost at iteration 60: 10362.488185\n",
      "Error rate: 0.062\n",
      "calculate_cost at iteration 70: 10532.722247\n",
      "Error rate: 0.065\n",
      "calculate_cost at iteration 80: 10150.408709\n",
      "Error rate: 0.061\n",
      "calculate_cost at iteration 90: 9911.378655\n",
      "Error rate: 0.06\n",
      "calculate_cost at iteration 100: 9792.461349\n",
      "Error rate: 0.06\n",
      "calculate_cost at iteration 110: 9696.671193\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 120: 9610.623551\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 130: 9531.519113\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 140: 9459.090883\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 150: 9393.197947\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 160: 9333.221047\n",
      "Error rate: 0.058\n",
      "calculate_cost at iteration 170: 9278.247714\n",
      "Error rate: 0.058\n",
      "calculate_cost at iteration 180: 9227.371848\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 190: 9179.868058\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 200: 9135.219915\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 210: 9093.067209\n",
      "Error rate: 0.06\n",
      "calculate_cost at iteration 220: 9053.143391\n",
      "Error rate: 0.06\n",
      "calculate_cost at iteration 230: 9015.234861\n",
      "Error rate: 0.059\n",
      "calculate_cost at iteration 240: 8979.160997\n",
      "Error rate: 0.06\n",
      "calculate_cost at iteration 250: 8944.765100\n",
      "Error rate: 0.06\n",
      "calculate_cost at iteration 260: 8911.909646\n",
      "Error rate: 0.062\n",
      "calculate_cost at iteration 270: 8880.473157\n",
      "Error rate: 0.061\n",
      "calculate_cost at iteration 280: 8850.347790\n",
      "Error rate: 0.063\n",
      "calculate_cost at iteration 290: 8821.437387\n",
      "Error rate: 0.064\n",
      "calculate_cost at iteration 300: 8793.655862\n",
      "Error rate: 0.064\n",
      "calculate_cost at iteration 310: 8766.925862\n",
      "Error rate: 0.064\n",
      "calculate_cost at iteration 320: 8741.177634\n",
      "Error rate: 0.064\n",
      "calculate_cost at iteration 330: 8716.348076\n",
      "Error rate: 0.064\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    Fun_GradientDecent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
